{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6589fc93-39d1-4d10-be1f-e7eb33fe4087",
   "metadata": {},
   "source": [
    "# Finding Missing child items and Improving Line items grouping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf22bf7-4a47-4f3a-9eef-6f19348a5250",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f188e-fe11-4a49-b7c8-080e0e69ce7a",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036937a-0221-48eb-862e-3fa0b8e646a8",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of the tool is to find the missing child items and group the correct child items into parent line items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a4e82-5e83-468a-b0e5-097ca14f15d5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Vertex AI Notebook Or Colab (If using Colab, use authentication)\n",
    "* Storage Bucket for storing input and output json files\n",
    "* Permission For Google Storage and Vertex AI Notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81de40-5c62-4c0b-adea-937f957b1a6e",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142123d3-37b1-4aa8-841c-40c3bd52d70c",
   "metadata": {},
   "source": [
    "### 1. Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0643c5f9-29fe-4252-9e6c-e2afc8c2f2b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: google-cloud-documentai==2.16.0 in /opt/conda/lib/python3.10/site-packages (2.16.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai==2.16.0) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-documentai==2.16.0) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-documentai==2.16.0) (3.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.23.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai==2.16.0) (1.61.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai==2.16.0) (1.59.2)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai==2.16.0) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.5.0)\n",
      "--2024-01-30 10:18:21--  https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29700 (29K) [text/plain]\n",
      "Saving to: ‘utilities.py’\n",
      "\n",
      "utilities.py        100%[===================>]  29.00K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-01-30 10:18:21 (17.4 MB/s) - ‘utilities.py’ saved [29700/29700]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy google-cloud-storage google-cloud-documentai==2.16.0 \n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588c13e-0e09-4a76-8c21-85a68ee262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from tqdm import tqdm\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Any,Tuple\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c8c4c-68b8-413c-b4bc-c66f044d3b7a",
   "metadata": {},
   "source": [
    "### 2. Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb2160-f5b4-42da-acdb-1ba4babc2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the raw parsed JSON files. The path must end with a forward slash ('/').\n",
    "Gcs_input_path = \"gs://xxxxx/xxxxxxxxxxxx/xx/\"\n",
    "# Your Google Cloud project ID.\n",
    "project_id = 'xxx-xxxx-xxxx'\n",
    "# Path for saving the processed output files. Do not include a trailing forward slash ('/').\n",
    "Gcs_output_path = \"gs://xxxxx/xxxxxxxxxxxx/xx\"\n",
    "parent_type='table_item'\n",
    "Missing_items_flag='True' # case sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d4909-e00c-4704-a43b-6534f7403872",
   "metadata": {},
   "source": [
    "* ``Gcs_input_path ``: GCS Input Path. It should contain DocAI processed output json files. \n",
    "* ``Gcs_output_path ``: GCS Output Path. The updated jsons will be saved in output path. \n",
    "* ``project_id`` : It should contains the project id of your current project.\n",
    "* ``Missing_items_flag``:  \"True\" if we need to find the missing child items , missing items step will be skipped if this value is other than True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d1c70-fef5-49e3-a266-695bf8076a54",
   "metadata": {},
   "source": [
    "### 3. Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc3e35-e12a-40c6-9f81-10460a9a9421",
   "metadata": {},
   "source": [
    "### Note\n",
    "* While using the missing items code , if the line items are closer then `modify the get_token_data function by increasing or decreasing the x and y allowances`.\n",
    "* Human review is recomended after this tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c22bfdd-abdc-4d1c-8f7c-86164e7c4103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://test_vb1/1_Testing_2024/Test_gateless/FEB1/parsed_jsons/6422400092316209525/0/02a6179e-9762-4f0f-b7b3-fcb0da787e54_BankStatement_gateless-redacted-0.json\n",
      "0\n",
      "2\n",
      "gs://test_vb1/1_Testing_2024/Test_gateless/FEB1/parsed_jsons/6422400092316209525/1/02b28d2f-4026-4f2c-94da-fc53bd374517_BankStatement_gateless-redacted-0.json\n",
      "0\n",
      "[0.47736263275146484, 0.5094505548477173, 0.5406593680381775, 0.5705494284629822, 0.5942857265472412]\n",
      "2\n",
      "[0.47780218720436096, 0.508571445941925, 0.5406593680381775, 0.5701099038124084, 0.6000000238418579, 0.6307692527770996, 0.6593406796455383]\n",
      "3\n",
      "[0.04879120737314224, 0.08087912201881409, 0.11120878905057907, 0.14241757988929749, 0.16923077404499054]\n",
      "gs://test_vb1/1_Testing_2024/Test_gateless/FEB1/parsed_jsons/6422400092316209525/10/04410ed1-8857-42b3-8373-1ea957812171_BankStatement_gateless-redacted-0.json\n",
      "1\n",
      "[0.1428571492433548, 0.15736263990402222, 0.17186813056468964, 0.1850549429655075, 0.1986813247203827, 0.21450549364089966, 0.22901098430156708, 0.24307692050933838, 0.2549450695514679]\n",
      "0\n",
      "[0.5573626160621643, 0.5718681216239929, 0.696703314781189, 0.7252747416496277, 0.7393406629562378, 0.7520878911018372, 0.7665933966636658, 0.7802197933197021, 0.7947252988815308, 0.8083516359329224, 0.8224175572395325, 0.8364835381507874, 0.8509889841079712, 0.878241777420044, 0.8927472233772278, 0.9068132042884827, 0.921758234500885]\n",
      "gs://test_vb1/1_Testing_2024/Test_gateless/FEB1/parsed_jsons/6422400092316209525/100/c752e836-040f-4bf9-821b-bc0cfb05c9e8_BankStatement_gateless-redacted-0.json\n",
      "0\n",
      "[0.6268131732940674, 0.6426373720169067, 0.6593406796455383, 0.6764835119247437, 0.6940659284591675, 0.7090110182762146, 0.7265934348106384, 0.7428571581840515, 0.7595604658126831, 0.7771428823471069, 0.7898901104927063]\n",
      "2\n",
      "[0.30373626947402954, 0.32087913155555725, 0.3380219638347626, 0.3534066081047058, 0.37054944038391113, 0.3868131935596466, 0.4035164713859558, 0.4171428680419922, 0.43208789825439453, 0.4399999976158142, 0.44659340381622314, 0.46329671144485474, 0.47999998927116394, 0.4958241879940033, 0.512967050075531, 0.5292307734489441, 0.5441758036613464, 0.5745055079460144, 0.5920879244804382, 0.6083516478538513, 0.6241758465766907, 0.6334065794944763, 0.6549450755119324, 0.6720879077911377, 0.6874725222587585, 0.717362642288208, 0.7243956327438354, 0.7318681478500366, 0.746813178062439, 0.7635164856910706, 0.7802197933197021, 0.8101099133491516, 0.8259340524673462, 0.8430769443511963, 0.8505494594573975, 0.8562637567520142, 0.8720878958702087, 0.8887912034988403, 0.9059340953826904, 0.9204395413398743]\n",
      "gs://test_vb1/1_Testing_2024/Test_gateless/FEB1/parsed_jsons/6422400092316209525/101/d7bcd5fb-0b44-4aac-9f3c-d43c1c5f10be_BankStatement_gateless-redacted-0.json\n",
      "0\n",
      "gs://test_vb1/1_Testing_2024/Test_gateless/FEB1/parsed_jsons/6422400092316209525/102/db5a5660-5efd-4001-9da3-677ee8c0a576_BankStatement_gateless-redacted-0.json\n",
      "0\n",
      "[0.3591208755970001, 0.370109885931015, 0.38505494594573975, 0.5274725556373596, 0.5415384769439697, 0.567472517490387, 0.5907692313194275, 0.6158241629600525, 0.6298900842666626, 0.6417582631111145, 0.6690109968185425, 0.6927472352981567, 0.7076923251152039, 0.7331868410110474, 0.7578021883964539, 0.7832967042922974, 0.7947252988815308, 0.8074725270271301, 0.8215384483337402, 0.8465933799743652, 0.8712087869644165, 0.8857142925262451, 0.8997802138328552, 0.9098901152610779]\n",
      "1\n",
      "[0.06549450755119324, 0.089670330286026, 0.10241758078336716, 0.12835164368152618, 0.1538461595773697, 0.17802198231220245, 0.2043956071138382, 0.2298901081085205, 0.24307692050933838, 0.26813188195228577, 0.28175824880599976, 0.29582417011260986, 0.3081318736076355]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 589\u001b[0m\n\u001b[1;32m    586\u001b[0m         json_data\u001b[38;5;241m=\u001b[39m get_updated_grouped_line_items(json_data,parent_type)\n\u001b[1;32m    587\u001b[0m         store_document_as_json(documentai\u001b[38;5;241m.\u001b[39mDocument\u001b[38;5;241m.\u001b[39mto_json(json_data), Gcs_output_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m], (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(Gcs_output_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m3\u001b[39m:])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mfile_name_list[i])\n\u001b[0;32m--> 589\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 587\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    585\u001b[0m     json_data\u001b[38;5;241m=\u001b[39mget_missing_data(json_data,parent_type)\n\u001b[1;32m    586\u001b[0m json_data\u001b[38;5;241m=\u001b[39m get_updated_grouped_line_items(json_data,parent_type)\n\u001b[0;32m--> 587\u001b[0m \u001b[43mstore_document_as_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocumentai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGcs_output_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGcs_output_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile_name_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Untitled Folder/test/line_item_improver_bank_statements/Jan30/utilities.py:778\u001b[0m, in \u001b[0;36mstore_document_as_json\u001b[0;34m(document, bucket_name, file_name)\u001b[0m\n\u001b[1;32m    774\u001b[0m process_result_bucket \u001b[38;5;241m=\u001b[39m storage_client\u001b[38;5;241m.\u001b[39mget_bucket(bucket_name)\n\u001b[1;32m    775\u001b[0m document_blob \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mBlob(\n\u001b[1;32m    776\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(Path(file_name)), bucket\u001b[38;5;241m=\u001b[39mprocess_result_bucket\n\u001b[1;32m    777\u001b[0m )\n\u001b[0;32m--> 778\u001b[0m \u001b[43mdocument_blob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/blob.py:3056\u001b[0m, in \u001b[0;36mBlob.upload_from_string\u001b[0;34m(self, data, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   3054\u001b[0m data \u001b[38;5;241m=\u001b[39m _to_bytes(data, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3055\u001b[0m string_buffer \u001b[38;5;241m=\u001b[39m BytesIO(data)\n\u001b[0;32m-> 3056\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/blob.py:2763\u001b[0m, in \u001b[0;36mBlob.upload_from_file\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupload_from_file\u001b[39m(\n\u001b[1;32m   2625\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2626\u001b[0m     file_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2639\u001b[0m     retry\u001b[38;5;241m=\u001b[39mDEFAULT_RETRY_IF_GENERATION_SPECIFIED,\n\u001b[1;32m   2640\u001b[0m ):\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Upload the contents of this blob from a file-like object.\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m \n\u001b[1;32m   2643\u001b[0m \u001b[38;5;124;03m    The content type of the upload will be determined in order\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;124;03m             if the upload response returns an error status.\u001b[39;00m\n\u001b[1;32m   2762\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2763\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prep_and_do_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrewind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2766\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2778\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/blob.py:2604\u001b[0m, in \u001b[0;36mBlob._prep_and_do_upload\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   2601\u001b[0m predefined_acl \u001b[38;5;241m=\u001b[39m ACL\u001b[38;5;241m.\u001b[39mvalidate_predefined(predefined_acl)\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2604\u001b[0m     created_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2620\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(created_json)\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media\u001b[38;5;241m.\u001b[39mInvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/blob.py:2410\u001b[0m, in \u001b[0;36mBlob._do_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   2407\u001b[0m     retry \u001b[38;5;241m=\u001b[39m retry\u001b[38;5;241m.\u001b[39mget_retry_policy_if_conditions_met(query_params\u001b[38;5;241m=\u001b[39mquery_params)\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m _MAX_MULTIPART_SIZE:\n\u001b[0;32m-> 2410\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_multipart_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2427\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_resumable_upload(\n\u001b[1;32m   2428\u001b[0m         client,\n\u001b[1;32m   2429\u001b[0m         stream,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2441\u001b[0m         command\u001b[38;5;241m=\u001b[39mcommand,\n\u001b[1;32m   2442\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/storage/blob.py:1923\u001b[0m, in \u001b[0;36mBlob._do_multipart_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   1917\u001b[0m upload \u001b[38;5;241m=\u001b[39m MultipartUpload(upload_url, headers\u001b[38;5;241m=\u001b[39mheaders, checksum\u001b[38;5;241m=\u001b[39mchecksum)\n\u001b[1;32m   1919\u001b[0m upload\u001b[38;5;241m.\u001b[39m_retry_strategy \u001b[38;5;241m=\u001b[39m _api_core_retry_to_resumable_media_retry(\n\u001b[1;32m   1920\u001b[0m     retry, num_retries\n\u001b[1;32m   1921\u001b[0m )\n\u001b[0;32m-> 1923\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mupload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/resumable_media/requests/upload.py:153\u001b[0m, in \u001b[0;36mMultipartUpload.transmit\u001b[0;34m(self, transport, data, metadata, content_type, timeout)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(result)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/resumable_media/requests/_request_helpers.py:155\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    153\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    157\u001b[0m     error \u001b[38;5;241m=\u001b[39m e  \u001b[38;5;66;03m# Fall through to retry, if there are retries left.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/resumable_media/requests/upload.py:145\u001b[0m, in \u001b[0;36mMultipartUpload.transmit.<locals>.retriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretriable_request\u001b[39m():\n\u001b[0;32m--> 145\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(result)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/auth/transport/requests.py:542\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[0;32m--> 542\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAuthorizedSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_page_bbox(entity):\n",
    "    bound_poly = entity.page_anchor.page_refs\n",
    "    norm_ver = bound_poly[0].bounding_poly.normalized_vertices\n",
    "    x_values = [vertex.x for vertex in norm_ver]\n",
    "    y_values = [vertex.y for vertex in norm_ver]\n",
    "    bbox = [min(x_values), min(y_values), max(x_values), max(y_values)]\n",
    "\n",
    "    return bbox\n",
    "\n",
    "def get_page_wise_entities(json_dict):\n",
    "    \"\"\"Args: loaded json file\n",
    "    THIS FUNCTION GIVES THE ENTITIES SPEPERATED FROM EACH PAGE IN DICTIONARY FORMAT\n",
    "    RETURNS: {page: [entities]}\"\"\"\n",
    "    \n",
    "    entities_page={}\n",
    "    for entity in json_dict.entities:\n",
    "        page=entity.page_anchor.page_refs[0].page\n",
    "        if page in entities_page.keys():\n",
    "            entities_page[page].append(entity)\n",
    "        else:\n",
    "            entities_page[page]=[entity]\n",
    "            \n",
    "    return entities_page\n",
    "\n",
    "def  get_line_items_schema(line_items):\n",
    "    \n",
    "    # line_items = [entity for entity in json_dict.entities if entity.properties]\n",
    "    line_item_schema = []\n",
    "    schema_xy = []\n",
    "    for line_item in line_items:\n",
    "        temp_schema = {}\n",
    "        temp_xy = {}\n",
    "        for item in line_item.properties:\n",
    "            temp_schema[item.type] = temp_schema.get(item.type, 0) + 1\n",
    "            bbox = get_page_bbox(item)\n",
    "            if item.type in temp_xy:\n",
    "                temp_xy[item.type].append(bbox)\n",
    "            else:\n",
    "                temp_xy[item.type] = [bbox]\n",
    "\n",
    "            line_item_schema.append(temp_schema)\n",
    "            schema_xy.append(temp_xy)\n",
    "\n",
    "    flat_list = [(key, value) for item in line_item_schema for key, value in item.items()]\n",
    "\n",
    "    counter = Counter(dict(flat_list))\n",
    "    temp_schema_dict = dict(counter)\n",
    "    consolidated_positions_ent={}\n",
    "    x=[]\n",
    "    for k3,v3 in temp_schema_dict.items():\n",
    "        for l3 in schema_xy:\n",
    "            for k4,v4 in l3.items():\n",
    "                if k3==k4:\n",
    "                    for x12 in v4:\n",
    "                        if k3 in consolidated_positions_ent.keys():\n",
    "                            consolidated_positions_ent[k3].append(x12)\n",
    "                        else:\n",
    "                            consolidated_positions_ent[k3]=[x12]\n",
    "    final_ent_x12={}\n",
    "    final_ent_y12={}\n",
    "    for ent_typ,va1 in consolidated_positions_ent.items():\n",
    "        sorted_data = sorted(va1, key=lambda x: x[0])\n",
    "        groups = []\n",
    "        current_group = [sorted_data[0]]\n",
    "        difference_threshold = 0.02\n",
    "        for i in range(1, len(sorted_data)):\n",
    "            if abs(sorted_data[i][0] - current_group[-1][0]) <= difference_threshold:\n",
    "                current_group.append(sorted_data[i])\n",
    "            else:\n",
    "                groups.append(current_group)\n",
    "                current_group = [sorted_data[i]]\n",
    "        groups.append(current_group)\n",
    "        for va3 in  groups:\n",
    "            if len(va3)>=1:\n",
    "                if ent_typ in final_ent_x12.keys():\n",
    "                    final_ent_x12[ent_typ].append([min(item[0] for item in va3), max(item[2] for item in va3)])\n",
    "                    final_ent_y12[ent_typ].append([min(item[1] for item in va3), max(item[3] for item in va3)])\n",
    "                else:\n",
    "                    final_ent_x12[ent_typ]=[[min(item[0] for item in va3), max(item[2] for item in va3)]]\n",
    "                    final_ent_y12[ent_typ]=[[min(item[1] for item in va3), max(item[3] for item in va3)]]\n",
    "\n",
    "    return temp_schema_dict,final_ent_x12,final_ent_y12\n",
    "\n",
    "def get_token_xy(token: Any) -> Tuple[float, float, float, float]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts the normalized bounding box coordinates (min_x, min_y, max_x, max_y) of a token.\n",
    "\n",
    "    Args:\n",
    "    - token (Any): A token object with layout information.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[float, float, float, float]: The normalized bounding box coordinates.\n",
    "    \n",
    "    \"\"\"\n",
    "    vertices = token.layout.bounding_poly.normalized_vertices\n",
    "    minx_token, miny_token = min(point.x for point in vertices), min(point.y for point in vertices)\n",
    "    maxx_token, maxy_token = max(point.x for point in vertices), max(point.y for point in vertices)\n",
    "\n",
    "    return minx_token,miny_token,maxx_token,maxy_token\n",
    "\n",
    "def get_token_data(json_dict,min_x,max_x,min_y,max_y,page_num):\n",
    "    text_anc_temp=[]\n",
    "    text_anc=[]\n",
    "    page_anc_temp={'x':[],'y':[]}\n",
    "    y_allowance=0.01 # edit this if the line items are closer and your not getitng desir\n",
    "    x_allowance=0.02\n",
    "    for page in json_dict.pages:\n",
    "        if page_num==page.page_number-1:\n",
    "            for token in page.tokens:\n",
    "                minx_token,miny_token,maxx_token,maxy_token=get_token_xy(token)\n",
    "                if min_y<=miny_token+y_allowance and max_y>=maxy_token-y_allowance and min_x<=minx_token+x_allowance and max_x>=maxx_token-x_allowance:\n",
    "                    temp_anc=token.layout.text_anchor.text_segments[0]\n",
    "                    text_anc.append(temp_anc)\n",
    "                    page_anc_temp['x'].extend([minx_token,maxx_token])\n",
    "                    page_anc_temp['y'].extend([miny_token,maxy_token])\n",
    "                    for seg in token.layout.text_anchor.text_segments:\n",
    "                        text_anc_temp.append([seg.start_index,seg.end_index])\n",
    "    if page_anc_temp!={'x':[],'y':[]}:    \n",
    "        page_anc=[{'x':min(page_anc_temp['x']),'y':min(page_anc_temp['y'])},{'x':max(page_anc_temp['x']),'y':min(page_anc_temp['y'])},\n",
    "                  {'x':min(page_anc_temp['x']),'y':max(page_anc_temp['y'])},{'x':max(page_anc_temp['x']),'y':max(page_anc_temp['y'])}]\n",
    "    if text_anc_temp!=[]:\n",
    "        sorted_data = sorted(text_anc_temp, key=lambda x: x[0])\n",
    "        mention_text = \"\"\n",
    "        for start_index, end_index in sorted_data:\n",
    "            mention_text += json_dict.text[start_index:end_index] \n",
    "\n",
    "        return mention_text,text_anc,page_anc\n",
    "\n",
    "                \n",
    "\n",
    "def get_missing_fields(json_dict,line_items,temp_schema_dict,final_ent_x12,ent_x_region,line_item_y_region):\n",
    "    \n",
    "    for line_item in line_items:\n",
    "        import copy\n",
    "        page_num=0\n",
    "        temp_types=[]\n",
    "        mis_type=[]\n",
    "        text_anc_line=[]\n",
    "        text_anc_mt=[]\n",
    "        page_anc_line={'x':[],'y':[]}\n",
    "        deep_copy_temp_schema = copy.deepcopy(temp_schema_dict)\n",
    "\n",
    "        for child in line_item.properties:\n",
    "            temp_types.append(child.type)\n",
    "            for seg in child.text_anchor.text_segments:\n",
    "                text_anc_line.append(seg)\n",
    "                text_anc_mt.append([seg.start_index,seg.end_index])\n",
    "            for anc4 in child.page_anchor.page_refs:\n",
    "                # page_n=anc4.page\n",
    "                for xy2 in anc4.bounding_poly.normalized_vertices:\n",
    "                    page_anc_line['x'].append(xy2.x)\n",
    "                    page_anc_line['y'].append(xy2.y)\n",
    "        #only for bank statement parser output\n",
    "        for k2 in temp_types:\n",
    "            if 'deposit' in k2:\n",
    "                modified_schema = {key: value for key, value in deep_copy_temp_schema.items() if 'withdrawal' not in key}\n",
    "                break\n",
    "            elif 'withdrawal' in k2:\n",
    "                modified_schema = {key: value for key, value in deep_copy_temp_schema.items() if 'deposit' not in key}\n",
    "                break\n",
    "            if 'modified_schema' not in locals():\n",
    "                modified_schema=deep_copy_temp_schema\n",
    "\n",
    "        for t1,v1 in modified_schema.items():\n",
    "            if t1 in temp_types:\n",
    "                pass\n",
    "            else:\n",
    "                mis_type.append(t1)\n",
    "\n",
    "        if len(mis_type)>0:\n",
    "            for typ in mis_type:\n",
    "\n",
    "                for ent_pos in line_item.page_anchor.page_refs:\n",
    "                    page_num=ent_pos.page\n",
    "                    try:\n",
    "                        min_x=ent_x_region[typ][0]\n",
    "                    except:\n",
    "                        min_x=min(ver.x for ver in ent_pos.bounding_poly.normalized_vertices)\n",
    "                    min_y=min(ver.y for ver in ent_pos.bounding_poly.normalized_vertices)\n",
    "                    try:\n",
    "                        max_x=ent_x_region[typ][1]-0.02\n",
    "                    except:\n",
    "                        max_x=max(ver.x for ver in ent_pos.bounding_poly.normalized_vertices)-0.02\n",
    "                        \n",
    "                    if 'description' in typ:\n",
    "\n",
    "                        try:\n",
    "                            # differences = [line_item_y_region[i+1] - line_item_y_region[i] for i in range(len(line_item_y_region)-1)]\n",
    "                            # avg_dif=sum(differences) / len(differences)\n",
    "                            closest_index_y = min(range(len(line_item_y_region)), key=lambda i: abs(line_item_y_region[i] - min_y))\n",
    "#                             if abs(min_y-closest_index_y)>0.02:\n",
    "#                                 max_y=min_y+avg_dif\n",
    "#                                 print(max_y)\n",
    "#                             else:\n",
    "                                \n",
    "                            max_y=line_item_y_region[closest_index_y+1]\n",
    "                        except :\n",
    "                            pass\n",
    "                    else:\n",
    "                        max_y=max(ver.y for ver in ent_pos.bounding_poly.normalized_vertices)\n",
    "                        \n",
    "        \n",
    "                    try:\n",
    "                    \n",
    "                        mention_text,text_anc,page_anc=get_token_data(json_dict,min_x,max_x,min_y,max_y,page_num)\n",
    "                        for an3 in text_anc:\n",
    "                            text_anc_line.append(an3)\n",
    "                            text_anc_mt.append([an3.start_index,an3.end_index])\n",
    "                        for xy3 in page_anc:\n",
    "                            page_anc_line['x'].append(xy3['x'])\n",
    "                            page_anc_line['y'].append(xy3['y'])\n",
    "                        entity_new={'mention_text': mention_text,\n",
    "                                     'page_anchor': {'page_refs': [{'bounding_poly': {'normalized_vertices': page_anc},\n",
    "                                        'page': str(page_num)}]},\n",
    "                                     'text_anchor': {'content': mention_text,\n",
    "                                      'text_segments': text_anc},\n",
    "                                     'type': typ}\n",
    "                        line_item.properties.append(entity_new)\n",
    "                    except Exception as e:\n",
    "                        # print(e)\n",
    "                        # print('YES')\n",
    "                        pass\n",
    "        page_anc_final=[{'x':min(page_anc_line['x']),'y':min(page_anc_line['y'])},{'x':max(page_anc_line['x']),'y':min(page_anc_line['y'])},\n",
    "                      {'x':min(page_anc_line['x']),'y':max(page_anc_line['y'])},{'x':max(page_anc_line['x']),'y':max(page_anc_line['y'])}]\n",
    "        sorted_data_1 = sorted(text_anc_mt, key=lambda x: x[0])\n",
    "        mention_text_final = \"\"\n",
    "        for start_index_1, end_index_1 in sorted_data_1:\n",
    "            mention_text_final = mention_text_final+' '+json_dict.text[start_index_1:end_index_1] \n",
    "\n",
    "        line_item.mention_text=mention_text_final\n",
    "        for anc6 in line_item.page_anchor.page_refs:\n",
    "            anc6.bounding_poly.normalized_vertices=page_anc_final\n",
    "        line_item.text_anchor.text_segments=text_anc_line\n",
    "\n",
    "    new_ent=[]\n",
    "\n",
    "    for l1 in line_items:\n",
    "        new_ent.append(l1)\n",
    "    \n",
    "    return new_ent\n",
    "\n",
    "def get_schema_with_bbox(line_items):\n",
    "    line_item_schema = []\n",
    "    schema_xy = []\n",
    "    for line_item in line_items:\n",
    "        temp_schema = {}\n",
    "        temp_xy = {}\n",
    "        for item in line_item.properties:\n",
    "            temp_schema[item.type] = temp_schema.get(item.type, 0) + 1\n",
    "            bbox = get_page_bbox(item)\n",
    "            if item.type in temp_xy:\n",
    "                temp_xy[item.type].append(bbox)\n",
    "            else:\n",
    "                temp_xy[item.type] = [bbox]\n",
    "\n",
    "        line_item_schema.append(temp_schema)\n",
    "        schema_xy.append(temp_xy)\n",
    "    \n",
    "    return line_item_schema,schema_xy\n",
    "\n",
    "def get_anchor_entity(schema_xy,line_item_schema):\n",
    "    ent_y2 = {}\n",
    "    for sc1 in schema_xy:\n",
    "        for e2, bbox in sc1.items():\n",
    "            if len(bbox)==1:\n",
    "                for b2 in bbox:\n",
    "                    ent_y2.setdefault(e2, []).extend([b2[1], b2[3]])\n",
    "    #get the min and max y of entities\n",
    "    entity_min_max_y = {}\n",
    "    for en3, val3 in ent_y2.items():\n",
    "        min_y_3 = min(val3)\n",
    "        max_y_3 = max(val3)\n",
    "        entity_min_max_y[en3] = [min_y_3, max_y_3]\n",
    "    \n",
    "    #counting times the entity appeared uniquely in all the line items\n",
    "    entity_count = {}\n",
    "    for entry in line_item_schema:\n",
    "        for entity, value in entry.items():\n",
    "            if value == 1:\n",
    "                if entity in entity_count:\n",
    "                    entity_count[entity] += 1\n",
    "                else:\n",
    "                    entity_count[entity] = 1\n",
    "\n",
    "    value_counts = {}\n",
    "    for value in entity_count.values():\n",
    "        value_counts[value] = value_counts.get(value, 0) + 1\n",
    "    # Find the maximum value\n",
    "    max_value = max(value_counts.values())\n",
    "\n",
    "    # Find keys with the maximum value\n",
    "    keys_with_max_value = [key for key, value in value_counts.items() if value == max_value]\n",
    "\n",
    "    # Find the key with the maximum value (in case of ties, choose the maximum key)\n",
    "    max_key = max(keys_with_max_value)\n",
    "\n",
    "    repeated_key = [key for key, value in entity_count.items() if value == max_key]\n",
    "\n",
    "    filtered_entities = {key: entity_min_max_y[key] for key in repeated_key if key in entity_min_max_y}\n",
    "    # print(filtered_entities)\n",
    "    # print(max_key)\n",
    "    if len(filtered_entities) > 1 :\n",
    "        anchor_entity = min(filtered_entities, key=lambda k: filtered_entities[k][0])\n",
    "    else:\n",
    "        anchor_entity=list(filtered_entities.keys())[0]\n",
    "        \n",
    "    return anchor_entity\n",
    "    \n",
    "def entity_region_x(schema_xy):\n",
    "    def get_margin(min_y_bin,min_values=\"YES\"):\n",
    "        # Sort the list in ascending order\n",
    "        min_y_bin.sort()\n",
    "\n",
    "        bins = []\n",
    "        current_bin = [min_y_bin[0]]\n",
    "        # Iterate through the values to create bins\n",
    "        for i in range(1, len(min_y_bin)):\n",
    "            if min_y_bin[i] - current_bin[-1] < 0.05:\n",
    "                current_bin.append(min_y_bin[i])\n",
    "            else:\n",
    "                bins.append(current_bin.copy())\n",
    "                current_bin = [min_y_bin[i]]\n",
    "                \n",
    "        # Add the last bin\n",
    "        bins.append(current_bin)\n",
    "        final_bins=[]\n",
    "        for bin_1 in bins:\n",
    "            if len(bin_1)>=2:\n",
    "                final_bins.append(bin_1)\n",
    "        if final_bins==[]:\n",
    "            for bin_1 in bins:\n",
    "                if len(bin_1)>=1:\n",
    "                    final_bins.append(bin_1)\n",
    "        if min_values=='YES':\n",
    "            return min(min(inner_list) for inner_list in final_bins)\n",
    "        else:\n",
    "            return max(max(inner_list) for inner_list in final_bins)\n",
    "        \n",
    "    ent_full_boundries={}\n",
    "    for line_1 in schema_xy:\n",
    "        for typ_1,bbox_1 in line_1.items():\n",
    "            if len(bbox_1)==1:\n",
    "                if typ_1 in ent_full_boundries.keys():\n",
    "                    ent_full_boundries[typ_1].append(bbox_1[0])\n",
    "                else:\n",
    "                    ent_full_boundries[typ_1]=bbox_1\n",
    "    ent_margins={}\n",
    "    for ent_typ_1,values_1 in ent_full_boundries.items():\n",
    "        min_x_bin=[]\n",
    "        min_y_bin=[]\n",
    "        max_x_bin=[]\n",
    "        max_y_bin=[]\n",
    "        min_check=len(values_1)\n",
    "        for bbox in values_1:\n",
    "            min_x_bin.append(bbox[0])\n",
    "            min_y_bin.append(bbox[1])\n",
    "            max_x_bin.append(bbox[2])\n",
    "            max_y_bin.append(bbox[3])\n",
    "        min_x=get_margin(min_x_bin,min_values=\"YES\")\n",
    "        min_y=get_margin(min_y_bin,min_values=\"YES\")\n",
    "        max_x=get_margin(max_x_bin,min_values=\"NO\")\n",
    "        max_y=get_margin(max_y_bin,min_values=\"NO\")\n",
    "\n",
    "        ent_margins[ent_typ_1]=[min_x,min_y,max_x,max_y]\n",
    "    \n",
    "    ent_margin_withdrawal={}\n",
    "    ent_margin_deposit={}\n",
    "    for ent_3,bbox_3 in ent_margins.items():\n",
    "        if 'withdrawal' in ent_3:\n",
    "            ent_margin_withdrawal[ent_3]=bbox_3\n",
    "        elif 'deposit' in ent_3:\n",
    "            ent_margin_deposit[ent_3]=bbox_3\n",
    "        else:\n",
    "            ent_margin_withdrawal[ent_3]=bbox_3\n",
    "            ent_margin_deposit[ent_3]=bbox_3\n",
    "\n",
    "    def get_x_region(ent_margin_withdrawal):\n",
    "        sorted_ent_margin_withdrawal = sorted_data = dict(sorted(ent_margin_withdrawal.items(), key=lambda x: x[1][0]))\n",
    "        ent_x_regions={}\n",
    "        keys_sorted=list(sorted_ent_margin_withdrawal.keys())\n",
    "        for n_1 in range(len(keys_sorted)):\n",
    "            if n_1<len(keys_sorted)-1:\n",
    "                if sorted_ent_margin_withdrawal[keys_sorted[n_1]][2]>sorted_ent_margin_withdrawal[keys_sorted[n_1+1]][0]:  \n",
    "                    ent_x_regions[keys_sorted[n_1]]=[sorted_ent_margin_withdrawal[keys_sorted[n_1]][0],sorted_ent_margin_withdrawal[keys_sorted[n_1]][2]]\n",
    "                else:\n",
    "                    ent_x_regions[keys_sorted[n_1]]=[sorted_ent_margin_withdrawal[keys_sorted[n_1]][0],sorted_ent_margin_withdrawal[keys_sorted[n_1+1]][0]]\n",
    "            else:\n",
    "                ent_x_regions[keys_sorted[n_1]]=[sorted_ent_margin_withdrawal[keys_sorted[n_1]][0],sorted_ent_margin_withdrawal[keys_sorted[n_1]][2]]\n",
    "\n",
    "        return ent_x_regions\n",
    "\n",
    "    withdrawal_x_region=get_x_region(ent_margin_withdrawal)\n",
    "    deposit_x_region=get_x_region(ent_margin_deposit)\n",
    "    ent_x_region = {**deposit_x_region, **withdrawal_x_region}\n",
    "\n",
    "    \n",
    "    return ent_x_region\n",
    "\n",
    "def get_line_item_y_region(line_items):\n",
    "    line_item_y_region=[]\n",
    "    max_y_line_item=[]\n",
    "    for tab_item in line_items:\n",
    "        y_1=[]\n",
    "        for line_details in tab_item.page_anchor.page_refs:\n",
    "            page=line_details.page\n",
    "            for xy_1 in line_details.bounding_poly.normalized_vertices:\n",
    "                y_1.append(xy_1.y)\n",
    "        line_item_y_region.append(min(y_1))\n",
    "        max_y_line_item.append(max(y_1))\n",
    "\n",
    "    line_item_y_region.append(max(max_y_line_item))\n",
    "    sorted_line_item_y_region=sorted(line_item_y_region)\n",
    "    \n",
    "    return sorted_line_item_y_region\n",
    "\n",
    "def get_line_item_y_region_by_anchor(line_items,anchor_entity):\n",
    "    y_max_anchor=[]\n",
    "    y_min_anchor=[]\n",
    "\n",
    "    for tab1_item in line_items:\n",
    "        for child in tab1_item.properties:\n",
    "            if child.type_==anchor_entity:\n",
    "                y_2=[]\n",
    "                for child_details in child.page_anchor.page_refs:\n",
    "                    for xy_2 in child_details.bounding_poly.normalized_vertices:\n",
    "                        y_2.append(xy_2.y)\n",
    "                y_max_anchor.append(max(y_2))\n",
    "                y_min_anchor.append(min(y_2))\n",
    "    sorted_y_max_anchor=sorted(y_max_anchor)\n",
    "    sorted_y_min_anchor=sorted(y_min_anchor)\n",
    "    sorted_y_max_anchor.append(sorted_y_min_anchor[0])\n",
    "    sorted_y_anchor=sorted(sorted_y_max_anchor)\n",
    "\n",
    "    return sorted_y_anchor\n",
    "\n",
    "def get_line_item_region(schema_xy, anchor_entity,line_items):\n",
    "    region_y=[]\n",
    "    for reg in schema_xy:\n",
    "        for e4,v4 in reg.items():\n",
    "            if 'date' in e4:# if e4==anchor_entity:\n",
    "                region_y.append(v4[0][1])\n",
    "    #Get line item total region and getting all child items into single list\n",
    "    bbox_line_y=[]\n",
    "    bbox_line_x=[]\n",
    "    child_items=[]\n",
    "    for line_item in line_items:\n",
    "        bbox_line=get_page_bbox(line_item)\n",
    "        bbox_line_y.extend([bbox_line[1],bbox_line[3]])\n",
    "        bbox_line_x.extend([bbox_line[0],bbox_line[2]])\n",
    "        for child in line_item.properties:\n",
    "            child_items.append(child)\n",
    "    line_item_start_y=min(bbox_line_y)\n",
    "    line_item_end_y=max(bbox_line_y)\n",
    "    \n",
    "        #getting Boundry for each line item\n",
    "    line_item_region=[]\n",
    "    region_y=sorted(region_y)\n",
    "    for r1 in range(len(region_y)):\n",
    "        if r1==0:\n",
    "            line_item_region.append([line_item_start_y,region_y[r1+1]])\n",
    "        elif r1==len(region_y)-1:\n",
    "            line_item_region.append([region_y[r1],line_item_end_y])\n",
    "        else:\n",
    "            line_item_region.append([region_y[r1],region_y[r1+1]])\n",
    "\n",
    "    return line_item_region,child_items\n",
    "\n",
    "\n",
    "def group_line_items(parent_type,child_items,page,line_item_region,json_dict):\n",
    "\n",
    "    grouped_line_items=[]\n",
    "    \n",
    "    for boundry in line_item_region:\n",
    "        line_item_temp={'mention_text':'','page_anchor': {'page_refs': [{'bounding_poly': {'normalized_vertices':[]},'page':page }]},'properties':[],'text_anchor': {'text_segments':[]},'type': parent_type}\n",
    "        text_anc_temp=[]\n",
    "        page_anc_temp={'x':[],'y':[]}\n",
    "        mt_temp=''\n",
    "        for child_1 in child_items:\n",
    "            bbox_temp=get_page_bbox(child_1)\n",
    "            if bbox_temp[1]>=boundry[0]-0.005 and bbox_temp[3]<=boundry[1]+0.005:\n",
    "                # print('entered')\n",
    "                line_item_temp['properties'].append(child_1)\n",
    "                page_anc_temp['x'].extend([bbox_temp[0],bbox_temp[2]])\n",
    "                page_anc_temp['y'].extend([bbox_temp[1],bbox_temp[3]])\n",
    "                seg_temp=child_1.text_anchor.text_segments\n",
    "                for seg in seg_temp:\n",
    "                    text_anc_temp.append({'start_index':str(seg.start_index),'end_index':str(seg.end_index)})\n",
    "        sorted_data = sorted(text_anc_temp, key=lambda x: int(x['end_index']))\n",
    "        for sort_text in sorted_data:\n",
    "            mt_temp=mt_temp+' '+json_dict.text[int(sort_text['start_index']):int(sort_text['end_index'])]\n",
    "        line_item_temp['text_anchor']['text_segments']=sorted_data\n",
    "        line_item_temp['mention_text']=mt_temp\n",
    "        # print(mt_temp)\n",
    "        line_item_temp['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices']=[{'x':min(page_anc_temp['x']),'y':min(page_anc_temp['y'])},\n",
    "                                                                                               {'x':max(page_anc_temp['x']),'y':min(page_anc_temp['y'])},\n",
    "                                                                                               {'x':max(page_anc_temp['x']),'y':max(page_anc_temp['y'])},\n",
    "                                                                                               {'x':min(page_anc_temp['x']),'y':max(page_anc_temp['y'])}]\n",
    "        grouped_line_items.append(line_item_temp)\n",
    "        \n",
    "    return grouped_line_items\n",
    "\n",
    "def get_updated_grouped_line_items(json_dict,parent_type):\n",
    "    final_line_items=[]\n",
    "    page_wise_ent=get_page_wise_entities(json_dict)\n",
    "    entities_ungrouped=[]\n",
    "    for page_num, ent in page_wise_ent.items():\n",
    "        try:\n",
    "            line_items = [entity for entity in ent if entity.properties and entity.type==parent_type]\n",
    "            try:\n",
    "                line_item_schema,schema_xy=get_schema_with_bbox(line_items)\n",
    "                anchor_entity=get_anchor_entity(schema_xy,line_item_schema)\n",
    "                line_item_region,child_items=get_line_item_region(schema_xy, anchor_entity,line_items)\n",
    "                grouped_line_items=group_line_items(parent_type,child_items,page_num,line_item_region,json_dict)\n",
    "                for item in grouped_line_items:\n",
    "                    final_line_items.append(item)\n",
    "            except:\n",
    "                entities_ungrouped.append(line_items)\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "    final_entities=[]\n",
    "    for en3 in json_dict.entities:\n",
    "        if en3.type!=parent_type:\n",
    "            final_entities.append(en3)\n",
    "    for lin_it in final_line_items:\n",
    "        final_entities.append(lin_it)\n",
    "    if len(entities_ungrouped)>0:\n",
    "        for item_1 in entities_ungrouped:\n",
    "            for item_2 in item_1:\n",
    "                final_entities.append(item_2)\n",
    "                \n",
    "    \n",
    "    json_dict.entities=final_entities\n",
    "\n",
    "    return json_dict\n",
    "\n",
    "def get_missing_data(json_dict,parent_type):\n",
    "    page_wise_ent=get_page_wise_entities(json_dict)\n",
    "    new_added_entities=[]\n",
    "    other_entities=[]\n",
    "    json_dict=get_updated_grouped_line_items(json_dict,parent_type)\n",
    "    for page_num, ent in page_wise_ent.items():\n",
    "        line_items = [entity for entity in ent if entity.properties and entity.type==parent_type]\n",
    "        line_items_other= [entity for entity in ent if entity.properties and entity.type!=parent_type]\n",
    "        for other_ent in line_items_other:\n",
    "            other_entities.append(other_ent)\n",
    "        # print(page_num)\n",
    "        if len(line_items)>2:\n",
    "            line_item_schema,schema_xy=get_schema_with_bbox(line_items)\n",
    "            ent_x_region=entity_region_x(schema_xy)\n",
    "            # print(ent_x_region)\n",
    "            anchor_entity=get_anchor_entity(schema_xy,line_item_schema)\n",
    "            # print(anchor_entity)\n",
    "            line_item_y_region= get_line_item_y_region(line_items)\n",
    "            temp_schema_dict,final_ent_x12,final_ent_y12=get_line_items_schema(line_items)\n",
    "            new_ent=get_missing_fields(json_dict,line_items,temp_schema_dict,final_ent_x12,ent_x_region,line_item_y_region)\n",
    "            for item in new_ent:\n",
    "                new_added_entities.append(item)\n",
    "        else:\n",
    "            for lin_it1 in line_items:\n",
    "                other_entities.append(lin_it1)\n",
    "    final_entities=[]\n",
    "    for en3 in json_dict.entities:\n",
    "        if en3.type!=parent_type:\n",
    "            final_entities.append(en3)\n",
    "    for lin_it in new_added_entities:\n",
    "        final_entities.append(lin_it)\n",
    "    for lin_it2 in other_entities:\n",
    "        final_entities.append(lin_it2)\n",
    "    json_dict.entities=final_entities\n",
    "    \n",
    "    return json_dict\n",
    "\n",
    "def main():\n",
    "    file_name_list,file_path_dict=file_names(Gcs_input_path)\n",
    "    for i in range(len(file_name_list)):\n",
    "        file_path='gs://'+Gcs_input_path.split('/')[2]+'/'+file_path_dict[file_name_list[i]]\n",
    "        print(file_path)\n",
    "        json_data=documentai_json_proto_downloader(file_path.split('/')[2],('/').join(file_path.split('/')[3:]))\n",
    "        if Missing_items_flag=='True':\n",
    "            json_data=get_missing_data(json_data,parent_type)\n",
    "        json_data= get_updated_grouped_line_items(json_data,parent_type)\n",
    "        store_document_as_json(documentai.Document.to_json(json_data), Gcs_output_path.split('/')[2], ('/').join(Gcs_output_path.split('/')[3:])+'/'+file_name_list[i])\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3db946-595e-47ca-902b-c9f8aae3fef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
