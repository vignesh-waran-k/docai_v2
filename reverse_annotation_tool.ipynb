{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fc9006-82e2-4731-b22c-c10bc0a14a26",
   "metadata": {},
   "source": [
    "# Reverse Annotation Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5f6da-b99a-47d3-a29b-354052bbf768",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239464e-0027-4e20-830d-6cc677952c72",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85811c9-4482-49c2-aa0e-865c83118277",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Reverse Annotation Tool helps in annotating or labeling the entities in the document based on the ocr text tokens. The notebook script expects the input file containing the name of entities in tabular format. And the first row is the header representing the entities that need to be labeled in every document. The script calls the processor and parses each of these input documents. The parsed document is then annotated if input entities are present in the document based on the OCR text tokens. The result is an output json file with updated entities and exported into a storage bucket path. This result json files can be imported into a processor to further check the annotations are existing as per the input file which was provided to the script prior the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f693c2-1e62-4ae2-b7a8-7e617545eece",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "* Vertex AI Notebook\n",
    "* Input csv file containing list of files to be labeled.\n",
    "* Document AI Processor\n",
    "* GCS bucket for processing of  the input documents and writing the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346448a4-47e7-4dcf-a7fd-ef7d4975092b",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5c17c-5ba6-4796-9189-5bba491afc92",
   "metadata": {},
   "source": [
    "### 1. Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78231b5b-61cc-4026-845a-b0ccaec905cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install configparser\n",
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b58cc-ddd8-433b-9765-67624452dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df724e77-e98c-4982-b463-a7dbf6d50c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv,json\n",
    "import utilities\n",
    "\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz,process\n",
    "from typing import List, Dict, Tuple, Union, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32b0a0-cd39-4cdb-b348-4782d869178c",
   "metadata": {},
   "source": [
    "### 2. Input details\n",
    "\n",
    "* **Config file Creation** : Below code snippet creates configuration file for the script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c2461-c855-4a0a-8a5c-26594be45049",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config file creation\n",
    "config = configparser.ConfigParser()\n",
    "# Add the structure to the file\n",
    "config.add_section('Parameters')\n",
    "config.set('Parameters', 'project_id', '')\n",
    "config.set('Parameters', 'processor_id', \"\")\n",
    "config.set('Parameters', 'processor_version', '')\n",
    "config.set('Parameters', 'input_bucket', 'gs://')\n",
    "config.set('Parameters', 'output_bucket', 'gs://')\n",
    "config.set('Parameters', 'location', 'us')\n",
    "# Write the new structure to the new file\n",
    "with open(r\"configfile.ini\", 'w') as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaeb24a-9ac9-4148-bdf3-a2b70a6f2201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Config file reading\n",
    "Path= \"configfile.ini\" #Enter the path of config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(Path)\n",
    "\n",
    "project_id = config.get('Parameters','project_id')\n",
    "processor_id = config.get('Parameters','processor_id')\n",
    "processor_version= config.get('Parameters','processor_version')\n",
    "input_bucket = config.get('Parameters','input_bucket')\n",
    "output_bucket = config.get('Parameters','output_bucket')\n",
    "location = config.get('Parameters','location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a430b7-0c76-42fa-ad52-9cf771ff17d2",
   "metadata": {},
   "source": [
    "Upon executing the above script, ‘configfile.ini’ file is created in the same directory and expects the user to input details as shown below.\n",
    "Enter the appropriate values into the config.ini files before executing the script.\n",
    "\n",
    "<img src=\"./Images/config_file.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0d0e3-eb24-471b-9d81-86bc9546d4b2",
   "metadata": {},
   "source": [
    "* **GCS Bucket** : Copy the list of input document files into the bucket path.\n",
    "* **InputData.csv file** : This is a schema file containing a tabular data with header row as name of the entities that needs to be identified and annotated in the document and the following rows are for each file whose values needs to be extracted. The below image shows the structure of the input file.\n",
    "\n",
    "<img src=\"./Images/version_4_input.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c8b99-b64d-48c8-ad82-c12163a290ad",
   "metadata": {},
   "source": [
    "**Note** : \n",
    "1. Ensure the name of this file is ‘inputData.csv’.\n",
    "2. Specify the correct filenames and its extensions(.pdf) in the inputData.csv file.\n",
    "3. The values must match the text present in the document. There can be a different format for date, but the date should be the same. \n",
    "4. Second row is for the ‘Type’ (data type) of the entity which is optional.\n",
    "5. If multiple values are present, then the tool tags each occurrence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67755185-6fc7-47fd-9323-45827f8ae38f",
   "metadata": {},
   "source": [
    "### 3.Run the Code\n",
    "\n",
    "Create the **config.ini** and **inputData.csv** files in the same directory where the script resides. Ensure the project details are updated for values and confirm the values match the text present in document and inputData.csv file for every document. Copy the documents that need to be processed in the GCS Bucket.\n",
    "The complete script can be found in the last section of the document. \n",
    "\n",
    "Included functionality for labeling multiple line items. Kindly generate a CSV file, as depicted below, for the annotation ground truth file, and ensure that it is located in the same folder with the name **\"input_data.csv.\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95773f41-130a-4553-bc7a-c139663d5743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_input_schema(readSchemaFileName : str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads an input schema from a CSV file.\n",
    "\n",
    "    Args:\n",
    "    - read_schema_file_name (str): Path to the CSV file containing the schema.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the schema data.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_schema = pd.read_csv(readSchemaFileName, dtype=str)\n",
    "    df_schema = df_schema.drop(df_schema[df_schema['FileNames'] == 'Type'].index)\n",
    "    df_schema.replace('', np.nan, inplace=True)\n",
    "    return df_schema\n",
    "\n",
    "readSchemaFileName = 'inputData.csv'\n",
    "df_schema = read_input_schema(readSchemaFileName)\n",
    "\n",
    "# Group by 'FileNames' and process each group\n",
    "grouped = df_schema.groupby('FileNames', as_index=False)\n",
    "processed_rows = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.ffill().bfill()  # Forward and backward fill to handle NaNs\n",
    "    combined_row = []\n",
    "\n",
    "    # Flatten the group into a single list\n",
    "    for row in group.itertuples(index=False):\n",
    "        combined_row.extend(row)\n",
    "\n",
    "    processed_rows.append(combined_row)    \n",
    "# # Column headers based on your original CSV structure\n",
    "headers = [header.strip() for header in pd.read_csv(readSchemaFileName, nrows=0).columns.tolist()]\n",
    "prefix = \"line_item/\"\n",
    "\n",
    "# Extract the part after 'line_item/' for each item that starts with the prefix\n",
    "unique_entities = [item.split('/')[-1] for item in headers if item.startswith(prefix)]\n",
    "\n",
    "processed_files = set()  # Set to keep track of processed FileNames\n",
    "\n",
    "Desc_merge_update='Yes'# update to Yes if you want to combine description within the line item, else NO#\n",
    "line_item_across_pages='Yes'# update to Yes if you want to group line items across pages#\n",
    "\n",
    "\n",
    "def get_token_range(jsonData : object) -> Dict:\n",
    "    \"\"\"\n",
    "    Gets the token ranges from the provided JSON data.\n",
    "\n",
    "    Args:\n",
    "    - json_data (object): JSON data containing page and token information.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing token ranges with page number and token number information.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenRange={}\n",
    "    for i in range(0, len(jsonData.pages)):\n",
    "        for j in range(0,len(jsonData.pages[i].tokens)):\n",
    "            pageNumber = i\n",
    "            tokenNumber = j\n",
    "            try:\n",
    "                startIndex = int(jsonData.pages[i].tokens[j].layout.text_anchor.text_segments[0].start_index)\n",
    "            except:\n",
    "                startIndex = 0\n",
    "            endIndex = int(jsonData.pages[i].tokens[j].layout.text_anchor.text_segments[0].end_index)\n",
    "            tokenRange[range(startIndex, endIndex)] = {'page_number': pageNumber, 'token_number': tokenNumber}\n",
    "    return tokenRange    \n",
    "\n",
    "def fix_page_anchor_entity(i : object, jsonData : object, tokenRange : Dict) -> object:\n",
    "    \"\"\"\n",
    "    Fixes the page anchor entity based on the provided JSON data and token range.\n",
    "\n",
    "    Args:\n",
    "    - i (object): Entity object to be fixed.\n",
    "    - jsonData (object): JSON data containing page and token information.\n",
    "    - tokenRange (Dict): Dictionary containing token ranges with page number and token number information.\n",
    "\n",
    "    Returns:\n",
    "    - object: Fixed entity object.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = int(i.text_anchor.text_segments[0].start_index)\n",
    "    end = int(i.text_anchor.text_segments[0].end_index) - 1\n",
    "\n",
    "    for j in tokenRange:\n",
    "        if start in j:\n",
    "            lowerToken = tokenRange[j]\n",
    "    for j in tokenRange:\n",
    "        if end in j:\n",
    "            upperToken = tokenRange[j]\n",
    "\n",
    "    lowerTokenData = jsonData.pages[int(lowerToken['page_number'])].tokens[int(lowerToken['token_number'])].layout.bounding_poly.normalized_vertices\n",
    "    upperTokenData = jsonData.pages[int(upperToken['page_number'])].tokens[int(upperToken['token_number'])].layout.bounding_poly.normalized_vertices\n",
    "    # for A\n",
    "    xA = float(lowerTokenData[0].x)\n",
    "    yA = float(lowerTokenData[0].y)\n",
    "    xA_ = float(upperTokenData[0].x)\n",
    "    yA_ = float(upperTokenData[0].y)\n",
    "    # for B\n",
    "    xB = float(lowerTokenData[1].x)\n",
    "    yB = float(lowerTokenData[1].y)\n",
    "    xB_ = float(upperTokenData[1].x)\n",
    "    yB_ = float(upperTokenData[1].y)\n",
    "    # for C\n",
    "    xC = float(lowerTokenData[2].x)\n",
    "    yC = float(lowerTokenData[2].y)\n",
    "    xC_ = float(upperTokenData[2].x)\n",
    "    yC_ = float(upperTokenData[2].y)\n",
    "    # for D\n",
    "    xD = float(lowerTokenData[3].x)\n",
    "    yD = float(lowerTokenData[3].y)\n",
    "    xD_ = float(upperTokenData[3].x)\n",
    "    yD_ = float(upperTokenData[3].y)\n",
    "\n",
    "    A = {'x': min(xA, xA_),'y': min(yA, yA_)}\n",
    "    B = {'x': max(xB, xB_),'y': min(yB, yB_)}\n",
    "    C = {'x': max(xC, xC_),'y': max(yC, yC_)}\n",
    "    D = {'x': min(xD, xD_),'y': max(yD, yD_)}\n",
    "    i.page_anchor.page_refs[0].bounding_poly.normalized_vertices = [A, B, C, D]\n",
    "    i.page_anchor.page_refs[0].page=lowerToken['page_number']\n",
    "    return i\n",
    "\n",
    "def create_entity(mention_text : str, type_ : str, m : Any) -> object:\n",
    "    \"\"\"\n",
    "    Creates a Document Entity based on the provided mention text, type, and match object.\n",
    "\n",
    "    Args:\n",
    "    - mention_text (str): The text to be mentioned in the entity.\n",
    "    - type_ (str): The type of the entity.\n",
    "    - m (Union[re.Match, None]): Match object representing the start and end indices of the mention text.\n",
    "\n",
    "    Returns:\n",
    "    - documentai.Document.Entity: The created Document Entity.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity = documentai.Document.Entity()\n",
    "    entity.mention_text = mention_text\n",
    "    entity.type = type_\n",
    "    normalizedVertices = []\n",
    "    pageRefs = []\n",
    "    pageRefs.append({\"bounding_poly\":{\"normalized_vertices\":normalizedVertices}})\n",
    "    entity.page_anchor = {\"page_refs\":pageRefs}\n",
    "    entity.text_anchor = {\"text_segments\":[{'start_index':str(m.start()),'end_index':str(m.end())}]}\n",
    "    return entity\n",
    "\n",
    "def modify_as_parent_entity(entity : object, parent_name : str) -> object:\n",
    "    \"\"\"\n",
    "    Modifies the provided entity as a parent entity with the given parent name.\n",
    "\n",
    "    Args:\n",
    "    - entity (documentai.Document.Entity): The entity to be modified.\n",
    "    - parent_name (str): The name of the parent entity.\n",
    "\n",
    "    Returns:\n",
    "    - documentai.Document.Entity: The modified parent entity.\n",
    "    \"\"\"\n",
    "    \n",
    "    parent_entity = documentai.Document.Entity()\n",
    "    parent_entity.mention_text = entity.mention_text\n",
    "    parent_entity.type_ = parent_name\n",
    "    parent_entity.properties = [entity]\n",
    "    parent_entity.page_anchor = entity.page_anchor\n",
    "    parent_entity.text_anchor = entity.text_anchor\n",
    "    return parent_entity\n",
    "\n",
    "def to_camel_case(snake_str : str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a snake_case string to CamelCase.\n",
    "\n",
    "    Args:\n",
    "    - snake_str (str): The snake_case string to be converted.\n",
    "\n",
    "    Returns:\n",
    "    - str: The CamelCase representation of the input string.\n",
    "    \"\"\"\n",
    "    \n",
    "    components = snake_str.split('_')\n",
    "    return components[0] + ''.join(x.title() for x in components[1:])\n",
    "\n",
    "def convert_keys_to_camel_case(obj : Any) -> Any:\n",
    "    \"\"\"\n",
    "    Recursively convert keys of a dictionary or a list of dictionaries to CamelCase.\n",
    "\n",
    "    Args:\n",
    "    - obj (Union[dict, list]): The input dictionary or list of dictionaries.\n",
    "\n",
    "    Returns:\n",
    "    - Union[dict, list]: The converted object with CamelCase keys.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        new_obj = {}\n",
    "        for key, value in obj.items():\n",
    "            new_key = to_camel_case(key)\n",
    "            new_obj[new_key] = convert_keys_to_camel_case(value)\n",
    "        return new_obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_keys_to_camel_case(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def line_item_check(entities,unique_entities):\n",
    "    \"\"\"\n",
    "    Checks line items within a list of entities based on unique entity types.\n",
    "\n",
    "    Args:\n",
    "        entities: A list of entities to be checked.\n",
    "        unique_entities: A list of unique entity types to be considered for checking.\n",
    "\n",
    "    Returns:\n",
    "        An integer representing the type of line item found.\n",
    "        1 for single line item, 2 for multiple, and 0 for none.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_types = [\n",
    "        subentity.type\n",
    "        for entity in entities\n",
    "        if hasattr(entity, \"properties\")\n",
    "        for subentity in entity.properties\n",
    "    ]\n",
    "\n",
    "    entity_counts = {unique: entity_types.count(unique) for unique in unique_entities}\n",
    "    multiple_entities_count = sum(count > 1 for count in entity_counts.values())\n",
    "\n",
    "    if any(count == 1 for count in entity_counts.values()):\n",
    "        return 1\n",
    "    elif multiple_entities_count and (\n",
    "        len(unique_entities) >= 3 or multiple_entities_count >= 1\n",
    "    ):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_normalized_vertices(normalized_vertices : List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get normalized vertices and return the final vertices.\n",
    "\n",
    "    Args:\n",
    "    - normalized_vertices (List[Dict]): List of dictionaries containing x and y coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - List[Dict]: Final vertices as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_x = min(normalized_vertices, key=lambda d: d.x).x\n",
    "    max_x = max(normalized_vertices, key=lambda d: d.x).x\n",
    "    min_y = min(normalized_vertices, key=lambda d: d.y).y\n",
    "    max_y = max(normalized_vertices, key=lambda d: d.y).y\n",
    "    vertices_final = [\n",
    "        {\"x\": min_x, \"y\": min_y},\n",
    "        {\"x\": min_x, \"y\": max_y},\n",
    "        {\"x\": max_x, \"y\": min_y},\n",
    "        {\"x\": max_x, \"y\": max_y},\n",
    "    ]\n",
    "    \n",
    "    return vertices_final\n",
    "    \n",
    "def single_line_item_merge(entities,page):\n",
    "    \"\"\"\n",
    "    Merges single line item entities into a unified line item.\n",
    "\n",
    "    Args:\n",
    "        entities: A list of entities to be merged.\n",
    "        page: The page number as a string where these entities are found.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the merged line item.\n",
    "    \"\"\"\n",
    "    \n",
    "    line_item_sub_entities = [\n",
    "        subentity\n",
    "        for entity in entities\n",
    "        if entity.type == \"line_item\"\n",
    "        for subentity in entity.properties\n",
    "    ]\n",
    "\n",
    "    text_anchors = [\n",
    "        item.text_anchor.text_segments[0] for item in line_item_sub_entities\n",
    "    ]\n",
    "    normalized_vertices = [\n",
    "        vertex\n",
    "        for item in line_item_sub_entities\n",
    "        for vertex in item.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "    ]\n",
    "    \n",
    "    vertices_final = get_normalized_vertices(normalized_vertices)\n",
    "\n",
    "    line_item = {\n",
    "        \"mention_text\": \" \".join(item.mention_text for item in line_item_sub_entities),\n",
    "        \"page_anchor\": {\n",
    "            \"page_refs\": [\n",
    "                {\"bounding_poly\": {\"normalized_vertices\": vertices_final}, \"page\": page}\n",
    "            ]\n",
    "        },\n",
    "        \"properties\": line_item_sub_entities,\n",
    "        \"text_anchor\": {\n",
    "            \"text_segments\": sorted(text_anchors, key=lambda x: int(x.end_index))\n",
    "        },\n",
    "        \"type\": \"line_item\",\n",
    "    }\n",
    "\n",
    "    return line_item\n",
    "\n",
    "def get_lineitems_grouped_pages_across(json_dict : object, schema : Dict = {}) -> object:\n",
    "    def get_line_items_temp_schema(json_dict : object) -> Dict:   \n",
    "        \"\"\"\n",
    "        Analyzes the structure of line items in a JSON document and creates a temporary schema\n",
    "        based on the observed types and their frequencies.\n",
    "\n",
    "        Args:\n",
    "            json_dict (object): The JSON document.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, int]]: A dictionary representing the temporary schema with\n",
    "            keys as line item identifiers and values as dictionaries of child types and their frequencies.\n",
    "        \"\"\"\n",
    "        \n",
    "        line_items = [\n",
    "            entity \n",
    "            for entity in json_dict.entities \n",
    "            if entity.properties and entity.type_ == 'line_item'\n",
    "        ]\n",
    "        \n",
    "        x=1\n",
    "        line_types = {}\n",
    "        for i in line_items:\n",
    "            line_types['line' + '_' + str(x)]={}\n",
    "            for child in i.properties:\n",
    "                if child.type_ in line_types['line'+'_'+str(x)].keys():\n",
    "                    line_types['line' + '_' + str(x)][child.type_]=line_types['line' + '_' + str(x)][child.type_]+1\n",
    "                else:\n",
    "                    line_types['line' + '_' + str(x)][child.type]=1\n",
    "            x+=1          \n",
    "        from collections import Counter\n",
    "        all_child=[]\n",
    "        for key, val in line_types.items():\n",
    "            all_child.append(val)\n",
    "        counts = {k: Counter([d[k] for d in all_child if k in d]) for k in set().union(*all_child)}\n",
    "        temp_schema = {k: max(counts[k], key=counts[k].get) for k in counts}\n",
    "        return temp_schema\n",
    "\n",
    "    if schema=={}:\n",
    "        schema=get_line_items_temp_schema(json_dict)\n",
    "    line_items_1=[]\n",
    "    for entity in json_dict.entities:\n",
    "        if entity.properties:\n",
    "            line_items_1.append(entity)\n",
    "    line_item_sorted_first={}\n",
    "    line_item_sorted_last={}\n",
    "    line_item_across_pages_first={}\n",
    "    line_item_across_pages_last={}\n",
    "    for li_1 in line_items_1:\n",
    "        try:\n",
    "            page=li_1.page_anchor.page_refs[0].page\n",
    "        except:\n",
    "            page=str(0)\n",
    "        max_y=max(vertex.y for vertex in li_1.page_anchor.page_refs[0].bounding_poly.normalized_vertices)\n",
    "        min_y=min(vertex.y for vertex in li_1.page_anchor.page_refs[0].bounding_poly.normalized_vertices)\n",
    "        if page in line_item_sorted_last.keys():\n",
    "            if line_item_sorted_last[page]>=max_y:\n",
    "                pass\n",
    "            else:\n",
    "                line_item_sorted_last[page]=max_y\n",
    "                line_item_across_pages_last[page]=li_1\n",
    "        else:\n",
    "            line_item_sorted_last[page]=max_y\n",
    "            line_item_across_pages_last[page]=li_1\n",
    "        if page in line_item_sorted_first.keys():\n",
    "            if line_item_sorted_first[page]<=min_y:\n",
    "                pass\n",
    "            else:\n",
    "                line_item_sorted_first[page]=min_y\n",
    "                line_item_across_pages_first[page]=li_1\n",
    "        else:\n",
    "            line_item_sorted_first[page]=min_y\n",
    "            line_item_across_pages_first[page]=li_1\n",
    "\n",
    "    groups_across={};p=0\n",
    "    for page_last,ent_last in line_item_across_pages_last.items():\n",
    "        try:\n",
    "            groups_across[p]=[line_item_across_pages_last[page_last],line_item_across_pages_first[str(int(page_last)+1)]]\n",
    "            p+=1\n",
    "        except KeyError:\n",
    "            pass   \n",
    "    #getting schema of each line item in groups\n",
    "    schema_across={}\n",
    "    for group,match in groups_across.items():\n",
    "        for i in range(len(match)):\n",
    "            for subitem in match[i]['properties']:\n",
    "                if group in schema_across.keys():\n",
    "                    if i in schema_across[group].keys():\n",
    "                        if subitem['type'] in schema_across[group][i].keys():\n",
    "                            schema_across[group][i][subitem['type']]+=1\n",
    "                        else:\n",
    "                            schema_across[group][i][subitem['type']]=1\n",
    "                    else:\n",
    "                        schema_across[group][i]={subitem['type']:1}\n",
    "                else:\n",
    "                    schema_across[group]={i:{subitem['type']:1}}\n",
    "    group_entites_spread={}\n",
    "    for selected_group,schema_ent in schema_across.items():\n",
    "        missing_ent_0={}\n",
    "        for key in schema.keys():\n",
    "            if key not in schema_ent[0]:\n",
    "                missing_ent_0[key]=schema[key]\n",
    "            else:\n",
    "                missing_ent_0[key]=schema[key]-schema_ent[0][key]\n",
    "        for k1,v1 in missing_ent_0.items():\n",
    "            if v1>0:\n",
    "                if k1 in schema_ent[1]:\n",
    "                    if schema_ent[1][k1]>schema[k1] or len(schema_ent[1])<(len(schema)/2):\n",
    "                        if selected_group in group_entites_spread.keys():\n",
    "                            group_entites_spread[selected_group][k1]=schema_ent[1][k1]-schema[k1]\n",
    "                        else:\n",
    "                            if len(schema_ent[1])<(len(schema)/2):\n",
    "                                group_entites_spread[selected_group]={k1:schema_ent[1][k1]}\n",
    "                            else:\n",
    "                                group_entites_spread[selected_group]={k1:schema_ent[1][k1]-schema[k1]}\n",
    "    def get_ent_schffle(group : str, index_1 : int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Shuffles entities within a specified group based on their minimum Y coordinates.\n",
    "\n",
    "        Args:\n",
    "            group (str): The name of the entity group.\n",
    "            index_1 (int): The index for accessing entities within the group.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: A list of shuffled entities within the specified group.\n",
    "        \"\"\"\n",
    "        \n",
    "        ent_min_y={}\n",
    "        ent_sort_ent={}\n",
    "        for sube1 in groups_across[group][index_1]['properties']:\n",
    "            if sube1['type'] in group_entites_spread[group].keys():\n",
    "                min_y_temp=min(vertex['y'] for vertex in sube1['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices'])\n",
    "                if sube1['type'] in ent_min_y.keys():\n",
    "                    ent_min_y[sube1['type']].append(min_y_temp)\n",
    "                    if sube1['type'] in ent_sort_ent.keys():\n",
    "                        ent_sort_ent[sube1['type']][min_y_temp]=sube1\n",
    "                    else:\n",
    "                        ent_sort_ent[sube1['type']]={min_y_temp:sube1}\n",
    "                else:\n",
    "                    ent_min_y[sube1['type']]=[min_y_temp]\n",
    "                    if sube1['type'] in ent_sort_ent.keys():\n",
    "                        ent_sort_ent[sube1['type']][min_y_temp]=sube1\n",
    "                    else:\n",
    "                        ent_sort_ent[sube1['type']]={min_y_temp:sube1}\n",
    "        sorted_ent_min_y={key: sorted(values) for key, values in ent_min_y.items()}\n",
    "        ent_shuffle=[]\n",
    "        for en1,val1 in sorted_ent_min_y.items():\n",
    "            b=0\n",
    "            for num in range(group_entites_spread[group][en1]):\n",
    "                for miny in range(len(sorted_ent_min_y[en1])):\n",
    "                    if num>=b:\n",
    "                        ent_shuffle.append(ent_sort_ent[en1][sorted_ent_min_y[en1][miny]])\n",
    "                        b+=1\n",
    "        return ent_shuffle\n",
    "\n",
    "    def ent_move(group : str, index_1 : int, index_0 : str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Move entities within a specified group from one index to another.\n",
    "\n",
    "        Args:\n",
    "            group (str): The name of the entity group.\n",
    "            index_1 (int): The source index from which entities are moved.\n",
    "            index_0 (str): The destination index to which entities are moved.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: A modified list of entities within the specified group after the move.\n",
    "        \"\"\"\n",
    "        \n",
    "        import copy\n",
    "        temp_group=copy.deepcopy(groups_across[group])\n",
    "        for ent_sh in ent_shuffle:\n",
    "            for sub_en3 in temp_group[index_1]['properties']:\n",
    "                if ent_sh['type']==sub_en3['type'] and ent_sh['text_anchor']==sub_en3['text_anchor']:\n",
    "                    temp_group[index_1]['properties'].remove(ent_sh)\n",
    "            temp_group[index_0]['properties'].append(ent_sh)\n",
    "            for t1 in ent_sh['page_anchor']['page_refs']:\n",
    "                temp_group[index_0]['page_anchor']['page_refs'].append(t1)\n",
    "            temp_group[index_0]['mention_text']=temp_group[index_0]['mention_text']+' '+ent_sh['mention_text']\n",
    "            for t2 in ent_sh['text_anchor']['text_segments']:\n",
    "                temp_group[index_0]['text_anchor']['text_segments'].append(t2)\n",
    "        return temp_group\n",
    "\n",
    "    def correct_page_text(temp_group_1 : List[Dict], index_1 : int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Correct the page text for a group of entities at a specified index.\n",
    "\n",
    "        Args:\n",
    "            temp_group_1 (List[Dict]): A list of entities within a group.\n",
    "            index_1 (int): The index of the entities to be corrected.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: A modified list of entities after correcting the page text.\n",
    "        \"\"\"\n",
    "        \n",
    "        temp_x=[]\n",
    "        temp_y=[]\n",
    "        temp_text_anc=[]\n",
    "        temp_mention_text=''\n",
    "        for suben in temp_group_1[index_1]['properties']:\n",
    "            for tex_an1 in suben['text_anchor']['text_segments']:\n",
    "                temp_text_anc.append(tex_an1)\n",
    "            for page_an1 in suben['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices']:\n",
    "                temp_x.append(page_an1['x'])\n",
    "                temp_y.append(page_an1['y'])\n",
    "        updated_ver=[{'x':min(temp_x),'y':min(temp_y)},{'x':max(temp_x),'y':max(temp_y)},{'x':min(temp_x),'y':max(temp_y)},{'x':max(temp_x),'y':min(temp_y)}]\n",
    "        sorted_temp_text_anc=sorted(temp_text_anc, key=lambda x: int(x['end_index']))\n",
    "        temp_group_1[index_1]['text_anchor']['text_segments']=sorted_temp_text_anc\n",
    "        temp_group_1[index_1]['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices']=updated_ver\n",
    "        for t5 in sorted_temp_text_anc:\n",
    "            s1=t5['start_index']\n",
    "            e1=t5['end_index']\n",
    "            temp_mention_text=temp_mention_text+' '+json_dict['text'][int(s1):int(e1)]\n",
    "        temp_group_1[index_1]['mention_text']=temp_mention_text\n",
    "        return temp_group_1\n",
    "\n",
    "    if len(group_entites_spread)>0:\n",
    "        for group, entity_move in group_entites_spread.items():\n",
    "            try:\n",
    "                page_1=groups_across[group][0]['page_anchor']['page_refs'][0]['page']\n",
    "            except:\n",
    "                page_1='0'\n",
    "            try:\n",
    "                page_2=groups_across[group][1]['page_anchor']['page_refs'][0]['page']\n",
    "            except:\n",
    "                page_2='0'\n",
    "            if page_1<page_2:\n",
    "                ent_shuffle=get_ent_schffle(group,1)\n",
    "                temp_group_1=ent_move(group,1,0)\n",
    "                if len(temp_group_1[1]['properties'])>0:\n",
    "                    temp_group_updated=correct_page_text(temp_group_1,1)\n",
    "                else:\n",
    "                    temp_group_updated=[temp_group_1[0]]\n",
    "            elif page_1>page_2:\n",
    "                ent_shuffle=get_ent_schffle(group,0)\n",
    "                temp_group_1=ent_move(group,0,1)\n",
    "                if len(temp_group_1[0]['properties'])>0:\n",
    "                    temp_group_updated=correct_page_text(temp_group_1,0)\n",
    "                else:\n",
    "                    temp_group_updated=[temp_group_1[1]]\n",
    "            for ent_remove in groups_across[group]:\n",
    "                json_dict.entities.remove(ent_remove)\n",
    "            for ent_add in temp_group_updated:\n",
    "                json_dict.entities.append(ent_add)\n",
    "    return json_dict\n",
    "\n",
    "def get_page_wise_entities(json_dict : object) -> Dict:\n",
    "    \"\"\"\n",
    "    Get entities grouped by page from the provided JSON dictionary.\n",
    "\n",
    "    Args:\n",
    "        json_dict (object): The input JSON dictionary containing entities.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[object]]: A dictionary where keys are page numbers and values are lists of entities on each page.\n",
    "    \"\"\"\n",
    "    \n",
    "    entities_page={}\n",
    "    for entity in json_dict.entities:\n",
    "        page='0'\n",
    "        try:\n",
    "            if entity.page_anchor.page_refs[0].page:\n",
    "                page=entity.page_anchor.page_refs[0].page\n",
    "            if page in entities_page.keys():\n",
    "                entities_page[page].append(entity)\n",
    "            else:\n",
    "                entities_page[page]=[entity]\n",
    "        except:\n",
    "            pass\n",
    "    return entities_page\n",
    "\n",
    "def multi_page_entites(entities_pagewise : object, page : int) -> Tuple[List,str]:\n",
    "    \"\"\"\n",
    "    Process multi-page entities.\n",
    "\n",
    "    Args:\n",
    "        entities_pagewise (object): Entities on a specific page.\n",
    "        page (int): Page number.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List, str]: List of line items, considered boundary entity.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_types=[]\n",
    "    line_item_sub_entities=[]\n",
    "    for entity in entities_pagewise:\n",
    "        if entity.properties:\n",
    "            if entity.type_ == 'line_item':\n",
    "                for subentity in entity.properties:\n",
    "                    entity_types.append(subentity.type_)\n",
    "                    line_item_sub_entities.append(subentity)\n",
    "        else:\n",
    "            entity_types.append(entity.type_)\n",
    "    line_items_multi_dict={}\n",
    "    for unique in unique_entities:\n",
    "        if entity_types.count(unique)>1:\n",
    "            line_items_multi_dict[unique]=entity_types.count(unique)\n",
    "    from collections import Counter\n",
    "    value_counts = Counter(line_items_multi_dict.values())\n",
    "    max_count = max(value_counts.values())\n",
    "    entity_types_keys = [key for key, value in line_items_multi_dict.items() if value_counts[value] == max_count]\n",
    "    dict_unique_ent = {\n",
    "        entity_type: [\n",
    "            subentity \n",
    "            for entity in entities_pagewise \n",
    "            if entity.properties \n",
    "            for subentity in entity.properties \n",
    "            if subentity.type_ == entity_type\n",
    "        ] \n",
    "        for entity_type in entity_types_keys\n",
    "    }\n",
    "\n",
    "    region_line_items={}\n",
    "    region_line_items_x={}\n",
    "    region_line_items_y={}\n",
    "    opt_region={}\n",
    "    for ent in dict_unique_ent:\n",
    "        region=[]\n",
    "        dict_x_y=[]\n",
    "        count_product_code=0\n",
    "        min_x_1=[]\n",
    "        min_y_1=[]\n",
    "        for item in dict_unique_ent[ent]:\n",
    "            x_y={}\n",
    "            x=[]\n",
    "            y=[]\n",
    "            x_min=''\n",
    "            y_min=''\n",
    "            count_product_code+=1\n",
    "            for i in item.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "                y.append(i.y)\n",
    "                x.append(i.x)\n",
    "            x_min=min(x)\n",
    "            y_min=min(y)\n",
    "            diff=max(y)-min(y)\n",
    "            x_y[count_product_code]=[{'x':min(x),'y':min(y)},{'x':max(x),'y':max(y)}]\n",
    "            dict_x_y.append(x_y)\n",
    "            min_x_1.append(x_min)\n",
    "            min_y_1.append(y_min)\n",
    "        region_line_items_x[ent]=min_x_1\n",
    "        region_line_items_y[ent]=min_y_1\n",
    "        sorted_lst_x_y = sorted(dict_x_y, key=lambda x: list(x.values())[0][0]['y'])\n",
    "        region_line_items[ent]=sorted_lst_x_y\n",
    "        opt_region[ent]=diff\n",
    "    sorted_region_line_y={key:sorted(values) for key, values in region_line_items_y.items()}\n",
    "    sorted_region_line_x={key:sorted(values) for key, values in region_line_items_x.items()}\n",
    "    regions_line_y_final={}\n",
    "    opt_region_ent={}\n",
    "    for region_line in sorted_region_line_y:\n",
    "        line_no=0\n",
    "        line_range={}\n",
    "        for i in range(len(sorted_region_line_y[region_line])):\n",
    "            line_no+=1\n",
    "            try:\n",
    "                pair=(sorted_region_line_y[region_line][i],sorted_region_line_y[region_line][i+1])\n",
    "            except IndexError:\n",
    "                pair=(sorted_region_line_y[region_line][i])\n",
    "            line_range[line_no]=pair\n",
    "        if region_line in regions_line_y_final.keys():\n",
    "            regions_line_y_final[region_line].append(line_range)\n",
    "        else:\n",
    "            regions_line_y_final[region_line]=[line_range]\n",
    "        opt_region_ent[region_line]=max(sorted_region_line_y[region_line])-min(sorted_region_line_y[region_line])\n",
    "    max_value = max(opt_region_ent.values())  # Find the maximum value\n",
    "    selected_values = [key for key, value in opt_region_ent.items() if abs(value - max_value) < 0.005]  # Select values satisfying the condition\n",
    "    if len(selected_values)>1:\n",
    "        considered_boundry_ent=''\n",
    "        final_y=1\n",
    "        for selected_ent in selected_values:\n",
    "            if final_y>min(sorted_region_line_y[selected_ent]):\n",
    "                final_y=min(sorted_region_line_y[selected_ent])\n",
    "                considered_boundry_ent=selected_ent\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        considered_boundry_ent=selected_values[0]\n",
    "    import copy\n",
    "    sub_entities_list=copy.deepcopy(line_item_sub_entities)\n",
    "    line_item_dict_final={}\n",
    "    sub_entities_categorized=[]\n",
    "    count=0\n",
    "    for subentity in sub_entities_list:\n",
    "        y_ent=[]\n",
    "        for ver in subentity.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "            y_ent.append(ver.y)\n",
    "        for line,region in regions_line_y_final[considered_boundry_ent][0].items():\n",
    "            try:\n",
    "                if (min(y_ent)>=region[0] or max(y_ent)>=region[0])  and (max(y_ent)<region[1]):\n",
    "                    if line in line_item_dict_final.keys():\n",
    "                        count=count+1\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line].append(subentity)\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "                    else:\n",
    "                        count=count+1\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line]=[subentity]\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "            except TypeError:\n",
    "                if min(y_ent)>=region:\n",
    "                    if line in line_item_dict_final.keys():\n",
    "                        count=count+1\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line].append(subentity)\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "                    else:\n",
    "                        count=count+1\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line]=[subentity]\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "\n",
    "    for item in sub_entities_list:\n",
    "        if item not in sub_entities_categorized:\n",
    "            y_ent=[]\n",
    "            for ver in item.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "                y_ent.append(ver.y)\n",
    "            diff_line={}\n",
    "            for line1, y1 in regions_line_y_final[considered_boundry_ent][0].items():\n",
    "                try:\n",
    "                    diff=abs(min(y_ent)-y1[0])\n",
    "                    diff_line[line1]=diff\n",
    "                except TypeError:\n",
    "                    diff=abs(min(y_ent)-y1)\n",
    "                    diff_line[line1]=diff\n",
    "\n",
    "            min_dist=min(diff_line.values())\n",
    "            line_item_2=[key for key, value in diff_line.items() if value == min_dist]\n",
    "            if line_item_2[0] in line_item_dict_final.keys():\n",
    "                line_item_dict_final[line_item_2[0]].append(item)\n",
    "                sub_entities_categorized.append(item)\n",
    "            else:\n",
    "                line_item_dict_final[line_item_2[0]]=[item]\n",
    "                sub_entities_categorized.append(item)\n",
    "    temp3 = []\n",
    "    for element in line_item_sub_entities:\n",
    "        if element not in sub_entities_categorized:\n",
    "            temp3.append(element)\n",
    "    if len(sub_entities_categorized)<len(line_item_sub_entities):\n",
    "        left_out=len(line_item_sub_entities)-len(sub_entities_categorized)\n",
    "        print('out of {} subentities,{}  are not yet classified '.format(len(line_item_sub_entities),left_out))\n",
    "        pass\n",
    "    elif len(sub_entities_categorized)==len(line_item_sub_entities):\n",
    "        print('All lineitems are classified')\n",
    "        pass\n",
    "    else:\n",
    "        print('something is wrong in classified')\n",
    "        pass\n",
    "\n",
    "    def create_lineitem(line_item_sub_entities : List, page : str) -> Dict:\n",
    "        \"\"\"\n",
    "        Create a line item from a list of sub-entities.\n",
    "\n",
    "        Args:\n",
    "            line_item_sub_entities (List[object]): The list of sub-entities to be included in the line item.\n",
    "            page (str): The page number.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Union[str, Dict[str, List[object]], List[object], Dict[str, List[object]]]]: The created line item.\n",
    "        \"\"\"\n",
    "        line_item={'mention_text':'','page_anchor': {'page_refs': [{'bounding_poly': {'normalized_vertices':[]},'page':page }]},'properties':[],'text_anchor': {'text_segments':[]},'type': 'line_item'}\n",
    "        text_anchors_sub_entities=[]\n",
    "        for item in line_item_sub_entities:\n",
    "            text_anchors_sub_entities.append(item.text_anchor.text_segments[0])\n",
    "        line_item['text_anchor']['text_segments'] = text_anchors_sub_entities\n",
    "        sorted_list_text1=sorted(text_anchors_sub_entities, key=lambda x: int(x.end_index))\n",
    "        line_item_mention_text=''\n",
    "        line_item_properties=[]\n",
    "        line_item_text_segments=[]\n",
    "        line_item_normalizedvertices=[]\n",
    "        subentities_classified=[]\n",
    "        for index in sorted_list_text1:\n",
    "            for item in line_item_sub_entities:\n",
    "                if index in item.text_anchor.text_segments:\n",
    "                    if item not in subentities_classified:\n",
    "                        subentities_classified.append(item)\n",
    "                        line_item_mention_text=line_item_mention_text+' '+ item.mention_text\n",
    "                        line_item_properties.append(item)\n",
    "                        line_item_text_segments.append(index)\n",
    "                        for i in item.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "                            line_item_normalizedvertices.append(i)              \n",
    "        line_item_normalizedvertices_final = get_normalized_vertices(line_item_normalizedvertices)\n",
    "        line_item['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices']=line_item_normalizedvertices_final\n",
    "        line_item['mention_text']=line_item_mention_text\n",
    "        line_item['properties']=line_item_properties\n",
    "        line_item['text_anchor']['text_segments']=line_item_text_segments\n",
    "        return line_item\n",
    "\n",
    "    line_items_classified=[]\n",
    "    for key, line_item_1 in line_item_dict_final.items():\n",
    "        line_item=create_lineitem(line_item_1,page)\n",
    "        line_items_classified.append(line_item)\n",
    "    return line_items_classified,considered_boundry_ent\n",
    "\n",
    "def merge_entities(json_dict : object) -> object:\n",
    "    \"\"\"\n",
    "    Merge entities in a page-wise manner.\n",
    "\n",
    "    Args:\n",
    "        json_dict (object): The JSON object containing entities.\n",
    "\n",
    "    Returns:\n",
    "        object: The updated JSON object after merging entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    entitites_page_wise=get_page_wise_entities(json_dict)\n",
    "    line_entities_classified_pagewise=[]\n",
    "    for page, entities in entitites_page_wise.items():\n",
    "        line_entities_temp=''\n",
    "        line_item_count=line_item_check(entities,unique_entities)\n",
    "        if line_item_count==1:\n",
    "            line_entities_temp=single_line_item_merge(entities,page)\n",
    "            line_entities_classified_pagewise.append(line_entities_temp)\n",
    "        elif line_item_count>1:\n",
    "            line_entities_temp,considered_boundry_ent=multi_page_entites(entities,page)\n",
    "            for ent1 in line_entities_temp:\n",
    "                line_entities_classified_pagewise.append(ent1)\n",
    "        elif line_item_count==0:\n",
    "            print('no line items')\n",
    "    final_entities=[]\n",
    "    if len(line_entities_classified_pagewise)==0:\n",
    "        pass\n",
    "    else:\n",
    "        for entity in json_dict.entities:\n",
    "            if entity.type_ != 'line_item':\n",
    "                final_entities.append(entity)\n",
    "        for ent in line_entities_classified_pagewise:\n",
    "            final_entities.append(ent)\n",
    "        json_dict.entities = final_entities\n",
    "\n",
    "    return json_dict\n",
    "\n",
    "def desc_merge_update(json_dict : object) -> object:\n",
    "    def desc_merge_1(ent_desc : List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Merge description entities.\n",
    "\n",
    "        Args:\n",
    "            ent_desc (List[Dict]): List of description entities.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Merged description entity.\n",
    "        \"\"\"\n",
    "        desc_merge={'mention_text':'','page_anchor': {'page_refs':''},'text_anchor': {'text_segments':[]},'type': 'line_item/description'}\n",
    "        text_anchors_desc_merge=[]\n",
    "        pagerefs=''\n",
    "        for item in ent_desc:\n",
    "            text_anchors_desc_merge.append(item['text_anchor']['text_segments'][0])\n",
    "            pagerefs=item['page_anchor']['page_refs']\n",
    "        desc_merge['page_anchor']['page_refs']=pagerefs\n",
    "        desc_merge['text_anchor']['text_segments']=text_anchors_desc_merge\n",
    "        sorted_list_text1=sorted(text_anchors_desc_merge, key=lambda x: int(x['end_index']))\n",
    "        desc_mention_text=''\n",
    "        desc_text_segments=[]\n",
    "        desc_normalizedvertices=[]\n",
    "        subentities_classified=[]\n",
    "        for index in sorted_list_text1:\n",
    "            for item in ent_desc:\n",
    "                if index in item['text_anchor']['text_segments']:\n",
    "                    if item not in subentities_classified:\n",
    "                        subentities_classified.append(item)\n",
    "                        desc_mention_text=(desc_mention_text+' '+ item['mention_text'])\n",
    "                        desc_text_segments.append(index)\n",
    "                        for i in item['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices']:\n",
    "                            desc_normalizedvertices.append(i)\n",
    "        desc_normalizedvertices_final = get_normalized_vertices(desc_normalizedvertices)\n",
    "        desc_merge['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices']=desc_normalizedvertices_final\n",
    "        desc_merge['mention_text']=desc_mention_text\n",
    "        desc_merge['text_anchor']['text_segments']=desc_text_segments\n",
    "        return desc_merge\n",
    "\n",
    "    for entity in json_dict.entities:\n",
    "        line_en=[]\n",
    "        ent_desc=[]\n",
    "        desc_ent_merge={}\n",
    "        if entity.type_ == 'line_item':\n",
    "            line_en.append(entity.properties)\n",
    "        for itm in line_en:\n",
    "            for ent1 in itm:\n",
    "                if ent1.type_ == 'line_item/description':\n",
    "                    ent_desc.append(ent1)\n",
    "        if len(ent_desc)>1:\n",
    "            desc_merge=desc_merge_1(ent_desc)\n",
    "            if entity.type_ == 'line_item':\n",
    "                for en2 in ent_desc:\n",
    "                    if en2 in entity.properties:\n",
    "                        del entity.properties[entity.properties.index(en2)]\n",
    "            entity.properties.append(desc_merge)\n",
    "    return json_dict\n",
    "\n",
    "def process_and_update_json(res_dict : object) -> object:\n",
    "    \"\"\"\n",
    "    Process and update the JSON dictionary.\n",
    "\n",
    "    Args:\n",
    "        res_dict (Dict): Input JSON dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Updated JSON dictionary.\n",
    "    \"\"\"\n",
    "    json_dict_updated = merge_entities(res_dict)\n",
    "    if line_item_across_pages == 'Yes':\n",
    "        json_dict_updated = get_lineitems_grouped_pages_across(json_dict_updated)\n",
    "    if Desc_merge_update == 'Yes':\n",
    "        json_dict_updated = desc_merge_update(json_dict_updated)\n",
    "    return json_dict_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710224d-9414-4a37-bc10-24667bed7f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in tqdm(processed_rows):\n",
    "    file_name = row[0]  # The first item is 'FileNames'\n",
    "\n",
    "    if file_name not in processed_files:\n",
    "        print('Processing:', file_name)\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(input_bucket.split('/')[2])\n",
    "        file_name_path=input_bucket+file_name\n",
    "        file_name_path=\"/\".join(file_name_path.split('/')[3:])\n",
    "        blob = bucket.blob(file_name_path)\n",
    "        content = blob.download_as_bytes()\n",
    "        res = utilities.process_document_sample(project_id=project_id, location=location, processor_id=processor_id, pdf_bytes = content, processor_version=processor_version)\n",
    "        res_dict = res.document\n",
    "        tokenRange = get_token_range(res_dict)\n",
    "\n",
    "        # Add the file_name to the set of processed files\n",
    "        processed_files.add(file_name)\n",
    "\n",
    "    list_of_entities = []\n",
    "    list_of_entities_not_mapped = []\n",
    "    processed_entities = set()  # Set to track processed entities\n",
    "    # Iterate over the sets of data associated with this file_name\n",
    "    for i in range(0, len(row), len(headers)):\n",
    "        row_slice = row[i:i + len(headers)]\n",
    "        for j in range(1, len(headers)):\n",
    "            type_ = headers[j]\n",
    "            mention_text = row_slice[j]                    \n",
    "            if '/' in type_:\n",
    "                # ADDS LINE ITEM ENTITY\n",
    "                if mention_text:\n",
    "                    parts = type_.split('/')\n",
    "                    parent_name = parts[0]\n",
    "                    type_ = parts[1] if len(parts) > 1 else None\n",
    "                    occurrences = re.finditer(re.escape(str(mention_text)) + r\"[ |\\,|\\n]\", res_dict.text)\n",
    "                    entity_flag = False\n",
    "                    for m in occurrences:\n",
    "                        start, end = m.start(), m.end()\n",
    "                        entity_id = (mention_text, start, end)  # Unique identifier for each entity\n",
    "\n",
    "                        if entity_id not in processed_entities:\n",
    "                            entity_flag = True\n",
    "                            entity = create_entity(mention_text, type_, m)\n",
    "                            try:\n",
    "                                entity_modified = fix_page_anchor_entity(entity, res_dict, tokenRange)\n",
    "                                entity_modified_as_parent = modify_as_parent_entity(entity_modified,parent_name)\n",
    "                                camel_case_entity = convert_keys_to_camel_case(entity_modified_as_parent)\n",
    "                                processed_entities.add(entity_id)  # Add the unique identifier to the set\n",
    "                                list_of_entities.append(camel_case_entity)\n",
    "                            except:\n",
    "                                print(\"Not able to find \" + mention_text + \" in the OCR\")\n",
    "                                continue\n",
    "                    if not entity_flag:\n",
    "                        list_of_entities_not_mapped.append(type_)\n",
    "                    continue    \n",
    "            else:\n",
    "                # ADDS Normal Entity\n",
    "                if mention_text:\n",
    "                    occurrences = re.finditer(re.escape(str(mention_text)) + r\"[ |\\,|\\n]\", res_dict.text)\n",
    "                    entity_flag = False\n",
    "\n",
    "                    for m in occurrences:\n",
    "                        start, end = m.start(), m.end()\n",
    "                        entity_id = (mention_text, start, end)  # Unique identifier for each entity\n",
    "\n",
    "                        if entity_id not in processed_entities:\n",
    "                            entity_flag = True\n",
    "                            entity = create_entity(mention_text, type_, m)\n",
    "                            try:\n",
    "                                entity_modified = fix_page_anchor_entity(entity, res_dict, tokenRange)\n",
    "                                camel_case_entity = convert_keys_to_camel_case(entity_modified)\n",
    "                                processed_entities.add(entity_id)  # Add the unique identifier to the set\n",
    "                                list_of_entities.append(camel_case_entity)\n",
    "                            except:\n",
    "                                print(\"Not able to find \" + mention_text + \" in the OCR\")\n",
    "                                continue\n",
    "                    if not entity_flag:\n",
    "                        list_of_entities_not_mapped.append(type_)\n",
    "\n",
    "    print(\"Number of entities that are mapped: \",len(list_of_entities))\n",
    "    res_dict.entities = list_of_entities\n",
    "\n",
    "    # Integrate with CODE 2 Processing\n",
    "    updated_json_dict = process_and_update_json(res_dict)\n",
    "\n",
    "    # Write the final output to GCS\n",
    "    output_bucket_name = output_bucket.split('/')[2]\n",
    "    output_path_within_bucket = '/'.join(output_bucket.split('/')[3:]) + file_name\n",
    "    utilities.store_document_as_json(documentai.Document.to_json(updated_json_dict), output_bucket_name, output_path_within_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4232a3-ec74-4786-b98c-df65d9b19495",
   "metadata": {},
   "source": [
    "### 4.Output\n",
    "\n",
    "* The output processed json files are available in the GCS bucket. These files can be further imported into the DocumentAI processor and checked for their annotation\n",
    "\n",
    "<img src=\"./Images/output_bucket.png\" width=800 height=400></img>\n",
    "\n",
    "* When the files are imported to the DocumentAI processor, the annotated entities are observed.The number for entities that were mapped are displayed during the runtime.\n",
    "\n",
    "<img src=\"./Images/output_2.png\" width=800 height=400></img>\n",
    "\n",
    "<img src=\"./Images/output_1.png\" width=800 height=400></img>\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
